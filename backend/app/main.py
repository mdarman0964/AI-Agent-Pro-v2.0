"""
AI Agent Pro v2.0 - Backend API
Universal Multi-Agent Project Generation with Real AI Integration
"""

import asyncio
import json
from typing import List, Optional, Dict, Any
from datetime import datetime
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Depends, Header, BackgroundTasks, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from pydantic import BaseModel, Field
import httpx
import aiohttp

# Models
class ProjectRequest(BaseModel):
    agentType: str = Field(..., description="Type of agent: android, python, uiux, fullstack, ml")
    projectName: str = Field(..., min_length=1, max_length=100)
    description: str = Field(..., min_length=10)
    features: List[str] = Field(default_factory=list)
    language: str = "kotlin"
    includeBackend: bool = False
    provider: str = "openai"
    model: Optional[str] = None
    api_key: str
    base_url: Optional[str] = None

class ChatMessage(BaseModel):
    role: str  # "user" or "assistant"
    content: str
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    provider: str
    api_key: str
    model: Optional[str] = None
    base_url: Optional[str] = None
    stream: bool = False

class GeneratedFile(BaseModel):
    path: str
    content: str
    language: str
    size: int

class ProjectResponse(BaseModel):
    success: bool
    message: str
    files: List[GeneratedFile]
    projectStructure: str
    tokens_used: Optional[int] = None
    generated_at: datetime = Field(default_factory=datetime.utcnow)

class GitHubPushRequest(BaseModel):
    token: str
    repo_name: str
    files: List[GeneratedFile]
    branch: str = "main"
    commit_message: str = "Generated by AI Agent Pro"

# Provider configurations
PROVIDERS = {
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "chat_endpoint": "/chat/completions",
        "default_model": "gpt-4-turbo-preview",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    },
    "moonshot": {
        "base_url": "https://api.moonshot.cn/v1",
        "chat_endpoint": "/chat/completions",
        "default_model": "kimi-2.5",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    },
    "deepseek": {
        "base_url": "https://api.deepseek.com/v1",
        "chat_endpoint": "/chat/completions",
        "default_model": "deepseek-chat",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    },
    "groq": {
        "base_url": "https://api.groq.com/openai/v1",
        "chat_endpoint": "/chat/completions",
        "default_model": "llama2-70b-4096",
        "headers": lambda key: {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
    },
    "anthropic": {
        "base_url": "https://api.anthropic.com",
        "chat_endpoint": "/v1/messages",
        "default_model": "claude-3-opus-20240229",
        "headers": lambda key: {"x-api-key": key, "Content-Type": "application/json", "anthropic-version": "2023-06-01"}
    },
    "gemini": {
        "base_url": "https://generativelanguage.googleapis.com/v1beta",
        "chat_endpoint": "/models/{model}:generateContent",
        "default_model": "gemini-pro",
        "headers": lambda key: {"Content-Type": "application/json"}
    }
}

# Prompt templates
PROMPT_TEMPLATES = {
    "android": """You are an expert Android developer. Generate a complete, production-ready Android project.

Project Name: {project_name}
Description: {description}
Features: {features}
Language: {language}

Generate the following files with complete implementation:
1. MainActivity.kt with proper MVVM architecture
2. ViewModel.kt with StateFlow
3. Repository.kt with error handling
4. XML layouts (activity_main.xml, item layouts)
5. build.gradle with all dependencies
6. AndroidManifest.xml

Format each file as:
===FILENAME: path/to/file.ext===
[complete file content]
===END===

Ensure:
- Follow Kotlin coding best practices
- Use Material Design 3 components
- Implement proper error handling
- Add necessary imports
- Use ViewBinding
- Follow official Android naming conventions""",

    "python": """You are an expert Python backend developer. Generate a complete FastAPI project.

Project Name: {project_name}
Description: {description}
Features: {features}

Generate the following files:
1. main.py with FastAPI app and all routes
2. models.py with SQLAlchemy models
3. schemas.py with Pydantic models
4. crud.py with database operations
5. database.py with connection handling
6. auth.py with JWT authentication
7. requirements.txt
8. Dockerfile
9. docker-compose.yml

Format each file as:
===FILENAME: path/to/file.ext===
[complete file content]
===END===

Ensure:
- Use type hints throughout
- Implement proper error handling
- Add async/await for I/O operations
- Include input validation
- Add security best practices""",

    "fullstack": """You are a full-stack developer. Generate both Android app and Python backend.

Project Name: {project_name}
Description: {description}
Features: {features}

Generate complete code for:
1. Android Frontend (Kotlin + MVVM)
2. Python Backend (FastAPI)
3. API integration between them
4. Shared data models

Format each file as:
===FILENAME: path/to/file.ext===
[complete file content]
===END===

Ensure proper integration between frontend and backend.""",

    "uiux": """You are a UI/UX designer. Generate beautiful Material Design 3 components.

Project Name: {project_name}
Description: {description}
Features: {features}

Generate:
1. XML layouts with ConstraintLayout
2. Custom views and animations
3. Color schemes and themes
4. Drawable resources
5. Animation XML files

Format each file as:
===FILENAME: path/to/file.ext===
[complete file content]
===END===""",

    "ml": """You are an ML engineer. Generate machine learning project code.

Project Name: {project_name}
Description: {description}
Features: {features}

Generate:
1. Model training script
2. Inference code
3. Data preprocessing
4. Model evaluation
5. Android integration (TFLite)

Format each file as:
===FILENAME: path/to/file.ext===
[complete file content]
===END==="""
}

# FastAPI app
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    print("ðŸš€ AI Agent Pro v2.0 Backend Starting...")
    yield
    # Shutdown
    print("ðŸ‘‹ Shutting down...")

app = FastAPI(
    title="AI Agent Pro v2.0 API",
    description="Universal Multi-Agent Project Generation with Real AI",
    version="2.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def call_llm_api(
    provider: str,
    api_key: str,
    messages: List[Dict[str, str]],
    model: Optional[str] = None,
    base_url: Optional[str] = None,
    stream: bool = False
) -> str:
    """Call LLM API with provider-specific formatting"""
    
    config = PROVIDERS.get(provider, PROVIDERS["openai"])
    url = (base_url or config["base_url"]) + config["chat_endpoint"]
    headers = config["headers"](api_key)
    model = model or config["default_model"]
    
    # Format messages based on provider
    if provider == "anthropic":
        # Anthropic uses different format
        system_msg = ""
        user_messages = []
        for msg in messages:
            if msg["role"] == "system":
                system_msg = msg["content"]
            else:
                user_messages.append({"role": msg["role"], "content": msg["content"]})
        
        payload = {
            "model": model,
            "messages": user_messages,
            "max_tokens": 4000,
            "system": system_msg
        }
    elif provider == "gemini":
        # Gemini uses different endpoint format
        url = url.format(model=model) + f"?key={api_key}"
        contents = [{"parts": [{"text": msg["content"]}]} for msg in messages if msg["role"] == "user"]
        payload = {"contents": contents}
    else:
        # OpenAI-compatible format
        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 4000,
            "stream": stream
        }
    
    async with httpx.AsyncClient(timeout=120.0) as client:
        try:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            
            # Extract content based on provider
            if provider == "anthropic":
                return data["content"][0]["text"]
            elif provider == "gemini":
                return data["candidates"][0]["content"]["parts"][0]["text"]
            else:
                return data["choices"][0]["message"]["content"]
                
        except httpx.HTTPStatusError as e:
            raise HTTPException(status_code=e.response.status_code, detail=f"API Error: {e.response.text}")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error calling LLM: {str(e)}")

def parse_generated_files(content: str) -> List[GeneratedFile]:
    """Parse AI response to extract files"""
    files = []
    lines = content.split('\n')
    current_file = None
    current_content = []
    
    for line in lines:
        if line.startswith('===FILENAME:'):
            if current_file:
                files.append(GeneratedFile(
                    path=current_file,
                    content='\n'.join(current_content).strip(),
                    language=get_language_from_path(current_file),
                    size=len('\n'.join(current_content).encode('utf-8'))
                ))
            current_file = line.replace('===FILENAME:', '').replace('===', '').strip()
            current_content = []
        elif line.startswith('===END==='):
            if current_file:
                files.append(GeneratedFile(
                    path=current_file,
                    content='\n'.join(current_content).strip(),
                    language=get_language_from_path(current_file),
                    size=len('\n'.join(current_content).encode('utf-8'))
                ))
            current_file = None
            current_content = []
        elif current_file is not None:
            current_content.append(line)
    
    return files

def get_language_from_path(path: str) -> str:
    """Detect programming language from file extension"""
    ext = path.split('.')[-1].lower() if '.' in path else ''
    language_map = {
        'kt': 'kotlin',
        'java': 'java',
        'py': 'python',
        'js': 'javascript',
        'ts': 'typescript',
        'xml': 'xml',
        'gradle': 'groovy',
        'json': 'json',
        'yaml': 'yaml',
        'yml': 'yaml',
        'md': 'markdown',
        'txt': 'text',
        'sh': 'bash',
        'dockerfile': 'dockerfile'
    }
    return language_map.get(ext, 'text')

def generate_project_structure(files: List[GeneratedFile], project_name: str) -> str:
    """Generate tree-like project structure"""
    structure = [f"{project_name}/"]
    
    # Group files by directory
    dirs: Dict[str, List[str]] = {}
    for file in files:
        path_parts = file.path.split('/')
        if len(path_parts) > 1:
            dir_path = '/'.join(path_parts[:-1])
            if dir_path not in dirs:
                dirs[dir_path] = []
            dirs[dir_path].append(path_parts[-1])
        else:
            if '.' not in dirs:
                dirs['.'] = []
            dirs['.'].append(file.path)
    
    # Build tree
    for dir_path, filenames in sorted(dirs.items()):
        if dir_path != '.':
            parts = dir_path.split('/')
            indent = '    ' * len(parts)
            structure.append(f"{indent}{parts[-1]}/")
            for filename in sorted(filenames):
                structure.append(f"{indent}    {filename}")
        else:
            for filename in sorted(filenames):
                structure.append(f"    {filename}")
    
    return '\n'.join(structure)

@app.post("/generate/project", response_model=ProjectResponse)
async def generate_project(request: ProjectRequest):
    """Generate complete project using AI"""
    
    try:
        # Get prompt template
        template = PROMPT_TEMPLATES.get(request.agentType, PROMPT_TEMPLATES["android"])
        
        # Format prompt
        prompt = template.format(
            project_name=request.projectName,
            description=request.description,
            features=', '.join(request.features) if request.features else 'None specified',
            language=request.language
        )
        
        # Prepare messages
        messages = [
            {"role": "system", "content": "You are an expert software developer. Generate complete, production-ready code files."},
            {"role": "user", "content": prompt}
        ]
        
        # Call LLM API
        response_content = await call_llm_api(
            provider=request.provider,
            api_key=request.api_key,
            messages=messages,
            model=request.model,
            base_url=request.base_url
        )
        
        # Parse generated files
        files = parse_generated_files(response_content)
        
        if not files:
            raise HTTPException(status_code=500, detail="No files generated. Check AI response format.")
        
        # Generate project structure
        structure = generate_project_structure(files, request.projectName)
        
        return ProjectResponse(
            success=True,
            message=f"Project '{request.projectName}' generated successfully using {request.provider}!",
            files=files,
            projectStructure=structure
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

@app.post("/chat")
async def chat(request: ChatRequest):
    """Chat with AI assistant"""
    
    try:
        messages = [{"role": msg.role, "content": msg.content} for msg in request.messages]
        
        response = await call_llm_api(
            provider=request.provider,
            api_key=request.api_key,
            messages=messages,
            model=request.model,
            base_url=request.base_url,
            stream=request.stream
        )
        
        return {"response": response, "timestamp": datetime.utcnow().isoformat()}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/github/push")
async def push_to_github(request: GitHubPushRequest):
    """Push generated files to GitHub"""
    
    try:
        from github import Github
        
        g = Github(request.token)
        user = g.get_user()
        
        # Create or get repo
        try:
            repo = user.get_repo(request.repo_name)
        except:
            repo = user.create_repo(
                request.repo_name,
                description="Generated by AI Agent Pro v2.0",
                private=True
            )
        
        # Get or create branch
        try:
            branch = repo.get_branch(request.branch)
        except:
            # Create from default branch
            default_branch = repo.get_branch(repo.default_branch)
            repo.create_git_ref(f"refs/heads/{request.branch}", default_branch.commit.sha)
        
        # Push files
        for file in request.files:
            try:
                # Check if file exists
                try:
                    existing = repo.get_contents(file.path, ref=request.branch)
                    repo.update_file(
                        file.path,
                        request.commit_message,
                        file.content,
                        existing.sha,
                        branch=request.branch
                    )
                except:
                    repo.create_file(
                        file.path,
                        request.commit_message,
                        file.content,
                        branch=request.branch
                    )
            except Exception as e:
                print(f"Error pushing {file.path}: {e}")
        
        return {
            "success": True,
            "message": f"Pushed {len(request.files)} files to {request.repo_name}/{request.branch}",
            "repo_url": repo.html_url
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GitHub push failed: {str(e)}")

@app.post("/download/zip")
async def download_zip(project_name: str, files: List[GeneratedFile]):
    """Create and return ZIP file of project"""
    
    import zipfile
    import io
    
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for file in files:
            zip_file.writestr(file.path, file.content)
    
    zip_buffer.seek(0)
    
    return StreamingResponse(
        zip_buffer,
        media_type="application/zip",
        headers={"Content-Disposition": f"attachment; filename={project_name}.zip"}
    )

@app.get("/providers")
def get_providers():
    """Get list of supported AI providers"""
    return {
        "providers": [
            {
                "id": key,
                "name": key.capitalize(),
                "default_model": config["default_model"],
                "base_url": config["base_url"]
            }
            for key, config in PROVIDERS.items()
        ]
    }

@app.get("/health")
def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "version": "2.0.0",
        "supported_providers": list(PROVIDERS.keys()),
        "timestamp": datetime.utcnow().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
